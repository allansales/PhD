return(w_wefat)
}
wefat <- function(targets, features, modelo){
get_w_wefat = function(col){
targets %>% group_by(get(col)) %>% summarise(w_wefat = w_wefat(get(col), features, modelo))
}
w_wefat_x = get_w_wefat("X")
w_wefat_y = get_w_wefat("Y")
w_wefat = bind_rows(w_wefat_x, w_wefat_y) %>% rename(palavra = "get(col)")
return(w_wefat)
}
wefat(targets, features, we_estadao_noticias)
x = c("psol","pmdb", "psdb")
y = c("psb", "pv", "pt")
a = c("dilma", "rousseff")
b = c("aécio", "neves")
targets = data_frame(X = x, Y = y)
features = data_frame(A = a, B = b)
w_wefat <- function(w, features, modelo){
numerador = score_w(w, features, modelo)
s_w = function(modelo, w, col){
features %>% group_by(get(col)) %>%
summarise(s = cosDist_model(modelo, w, get(col)))
}
s_a = s_w(modelo, w, "A")
print(s_a)
s_b = s_w(modelo, w, "B")
print(s_b)
w_wefat = bind_rows(s_a, s_b) %>% summarise(sd = sd(s)) %>% as.numeric()
return(w_wefat)
}
wefat <- function(targets, features, modelo){
get_w_wefat = function(col){
targets %>% group_by(get(col)) %>% summarise(w_wefat = w_wefat(get(col), features, modelo))
}
w_wefat_x = get_w_wefat("X")
w_wefat_y = get_w_wefat("Y")
w_wefat = bind_rows(w_wefat_x, w_wefat_y) %>% rename(palavra = "get(col)")
return(w_wefat)
}
wefat(targets, features, we_estadao_noticias)
w_wefat("pt", features, we_estadao_noticias)
w_wefat <- function(w, features, modelo){
numerador = score_w(w, features, modelo)
s_w = function(modelo, w, col){
features %>% group_by(get(col)) %>%
summarise(s = cosDist_model(modelo, w, get(col)))
}
s_a = s_w(modelo, w, "A")
s_b = s_w(modelo, w, "B")
w_wefat = bind_rows(s_a, s_b) %>% summarise(sd = sd(s)) %>% as.numeric()
return(w_wefat)
}
w_wefat("pt", features, we_estadao_noticias)
w_wefat("psdb", features, we_estadao_noticias)
cosDist_model <- function(modelo, w, col){
a = cosineDist(modelo[[w]], modelo[[col]])
print(a)
return(a)
}
w_wefat("psdb", features, we_estadao_noticias)
w_wefat <- function(w, features, modelo){
numerador = score_w(w, features, modelo)
s_w = function(modelo, w, col){
features %>% group_by(get(col)) %>%
summarise(s = cosDist_model(modelo, w, get(col)))
}
s_a = s_w(modelo, w, "A")
s_b = s_w(modelo, w, "B")
w_wefat = bind_rows(s_a, s_b) %>% summarise(sd = sd(s)) %>% as.numeric()
return(w_wefat)
}
cosDist_model <- function(modelo, w, col){
print(w)
print(col)
a = cosineDist(modelo[[w]], modelo[[col]])
print(a)
return(a)
}
w_wefat("psdb", features, we_estadao_noticias)
cosDist_model <- function(modelo, w, col){
print(w)
print(col)
a = cosineSimilarity(modelo[[w]], modelo[[col]])
print(a)
return(a)
}
score_w <- function(w, features, modelo){
mean_cosDist = function(w, col){
mean = features %>% group_by(get(col)) %>%
summarise(cosine = cosDist_model(modelo, w, get(col))) %>%
summarise(mean = mean(cosine))
return(mean)
}
mean_w_A = mean_cosDist(w, "A")
mean_w_B = mean_cosDist(w, "B")
return((mean_w_A - mean_w_B) %>% as.numeric())
}
score_targets <- function(targets, features, modelo){
sum_s_w = function(col, features, modelo){
targets %>% group_by(get(col)) %>%
summarise(s_w = score_w(get(col), features, modelo)) %>%
summarise(s = sum(s_w))
}
sum_w_X = sum_s_w("X", features, modelo)
sum_w_Y = sum_s_w("Y", features, modelo)
return(sum_w_X - sum_w_Y)
}
score_targets(targets, features, we_estadao_noticias)
cosDist_model <- function(modelo, w, col){
cosineSimilarity(modelo[[w]], modelo[[col]])
}
wefat(targets, features, we_estadao_noticias)
knitr::opts_chunk$set(echo = TRUE)
library("dplyr")
library("wordVectors")
library("partitions")
we_estadao_noticias = read.binary.vectors("../word_embeddings/eleicoes_2014/embeddings/eleicoes_2014_estadao.bin")
we_folha_noticias = read.binary.vectors("../word_embeddings/eleicoes_2014/embeddings/eleicoes_2014_folha.bin")
we_folha_comentarios = read.binary.vectors("../word_embeddings/eleicoes_2014/embeddings/folha_comentarios_eleicao_2014.bin")
x = c("psol","pmdb", "psdb")
y = c("psb", "pv", "pt")
a = c("dilma", "rousseff")
b = c("aécio", "neves")
targets = data_frame(X = x, Y = y)
features = data_frame(A = a, B = b)
# Calcula score de uma palavra para os conjuntos A e B
cosDist_model <- function(modelo, w, col){
cosineSimilarity(modelo[[w]], modelo[[col]])
}
score_w <- function(w, features, modelo){
mean_cosDist = function(w, col){
mean = features %>% group_by(get(col)) %>%
summarise(cosine = cosDist_model(modelo, w, get(col))) %>%
summarise(mean = mean(cosine))
return(mean)
}
mean_w_A = mean_cosDist(w, "A")
mean_w_B = mean_cosDist(w, "B")
return((mean_w_A - mean_w_B) %>% as.numeric())
}
score_targets <- function(targets, features, modelo){
sum_s_w = function(col, features, modelo){
targets %>% group_by(get(col)) %>%
summarise(s_w = score_w(get(col), features, modelo)) %>%
summarise(s = sum(s_w))
}
sum_w_X = sum_s_w("X", features, modelo)
sum_w_Y = sum_s_w("Y", features, modelo)
return(sum_w_X - sum_w_Y)
}
score_targets(targets, features, we_estadao_noticias)
## TO DO
effect_size <- function(targets, features, modelo){
s_w = function(col){
targets %>% group_by(get(col)) %>%
summarise(s_w = score_w(get(col), features, modelo))
}
x = s_w("X")
y = s_w("Y")
x_mean = x %>% summarise(mean = mean(s_w)) %>% as.numeric()
y_mean = y %>% summarise(mean = mean(s_w)) %>% as.numeric()
w_sd = bind_rows(x, y) %>% summarise(sd = sd(s_w)) %>% as.numeric()
return((x_mean - y_mean)/w_sd)
}
effect_size(targets, features, we_folha_noticias)
w_wefat <- function(w, features, modelo){
numerador = score_w(w, features, modelo)
s_w = function(modelo, w, col){
features %>% group_by(get(col)) %>%
summarise(s = cosDist_model(modelo, w, get(col)))
}
s_a = s_w(modelo, w, "A")
s_b = s_w(modelo, w, "B")
w_wefat = bind_rows(s_a, s_b) %>% summarise(sd = sd(s)) %>% as.numeric()
return(w_wefat)
}
w_wefat("psdb", features, we_estadao_noticias)
wefat <- function(targets, features, modelo){
get_w_wefat = function(col){
targets %>% group_by(get(col)) %>% summarise(w_wefat = w_wefat(get(col), features, modelo))
}
w_wefat_x = get_w_wefat("X")
w_wefat_y = get_w_wefat("Y")
w_wefat = bind_rows(w_wefat_x, w_wefat_y) %>% rename(palavra = "get(col)")
return(w_wefat)
}
wefat(targets, features, we_estadao_noticias)
w_wefat("psdb", features, we_estadao_noticias)
w_wefat("pt", features, we_estadao_noticias)
cosDist_model(we_estadao_noticias, "pt", "dilma")
cosDist_model(we_estadao_noticias, "pt", "aécio")
cosineSimilarity(we_estadao_noticias[["pt"]], we_estadao_noticias[["aécio"]])
cosineSimilarity(we_estadao_noticias[["pt"]], we_estadao_noticias[["aécio"]])
cosineSimilarity(we_estadao_noticias[["pt"]], we_estadao_noticias[["dilma"]])
cosDist_model(we_estadao_noticias, "psdb", "aécio")
cosineSimilarity(we_estadao_noticias[["pt"]], we_estadao_noticias[["dilma"]])
cosineSimilarity(we_estadao_noticias[["psdb"]], we_estadao_noticias[["aécio"]])
knitr::opts_chunk$set(echo = TRUE)
library("dplyr")
library("wordVectors")
library("partitions")
we_estadao_noticias = read.binary.vectors("../word_embeddings/eleicoes_2014/embeddings/eleicoes_2014_estadao.bin")
we_folha_noticias = read.binary.vectors("../word_embeddings/eleicoes_2014/embeddings/eleicoes_2014_folha.bin")
we_folha_comentarios = read.binary.vectors("../word_embeddings/eleicoes_2014/embeddings/folha_comentarios_eleicao_2014.bin")
x = c("psol","pmdb", "psdb")
y = c("psb", "pv", "pt")
a = c("dilma", "rousseff")
b = c("aécio", "neves")
targets = data_frame(X = x, Y = y)
features = data_frame(A = a, B = b)
# Calcula score de uma palavra para os conjuntos A e B
cosSim_model <- function(modelo, w, col){
cosineSimilarity(modelo[[w]], modelo[[col]])
}
score_w <- function(w, features, modelo){
mean_cosDist = function(w, col){
mean = features %>% group_by(get(col)) %>%
summarise(cosine = cosSim_model(modelo, w, get(col))) %>%
summarise(mean = mean(cosine))
return(mean)
}
mean_w_A = mean_cosDist(w, "A")
mean_w_B = mean_cosDist(w, "B")
return((mean_w_A - mean_w_B) %>% as.numeric())
}
score_targets <- function(targets, features, modelo){
sum_s_w = function(col, features, modelo){
targets %>% group_by(get(col)) %>%
summarise(s_w = score_w(get(col), features, modelo)) %>%
summarise(s = sum(s_w))
}
sum_w_X = sum_s_w("X", features, modelo)
sum_w_Y = sum_s_w("Y", features, modelo)
return(sum_w_X - sum_w_Y)
}
score_targets(targets, features, we_estadao_noticias)
## TO DO
effect_size <- function(targets, features, modelo){
s_w = function(col){
targets %>% group_by(get(col)) %>%
summarise(s_w = score_w(get(col), features, modelo))
}
x = s_w("X")
y = s_w("Y")
x_mean = x %>% summarise(mean = mean(s_w)) %>% as.numeric()
y_mean = y %>% summarise(mean = mean(s_w)) %>% as.numeric()
w_sd = bind_rows(x, y) %>% summarise(sd = sd(s_w)) %>% as.numeric()
return((x_mean - y_mean)/w_sd)
}
effect_size(targets, features, we_folha_noticias)
w_wefat <- function(w, features, modelo){
numerador = score_w(w, features, modelo)
s_w = function(modelo, w, col){
features %>% group_by(get(col)) %>%
summarise(s = cosSim_model(modelo, w, get(col)))
}
s_a = s_w(modelo, w, "A")
s_b = s_w(modelo, w, "B")
w_wefat = bind_rows(s_a, s_b) %>% summarise(sd = sd(s)) %>% as.numeric()
return(w_wefat)
}
wefat <- function(targets, features, modelo){
get_w_wefat = function(col){
targets %>% group_by(get(col)) %>% summarise(w_wefat = w_wefat(get(col), features, modelo))
}
w_wefat_x = get_w_wefat("X")
w_wefat_y = get_w_wefat("Y")
w_wefat = bind_rows(w_wefat_x, w_wefat_y) %>% rename(palavra = "get(col)")
return(w_wefat)
}
wefat(targets, features, we_estadao_noticias)
partidos <- c("pt","psdb","pmdb","ptb","pdt","dem","psb","ptc","psc","pmn","prp","pps","pv","pp","pstu","pcb","prtb","phs","psdc","pco","ptn","psl","prb","psol","ppl","pros","psd","sd", "pr","pen", "pcdob", "ptdob")
x <- c("pt","psdb","pmdb","ptb","pdt","dem","psb","ptc","psc","pmn","prp","pps","pv","pp","pstu","pcb")
y = c("phs","psdc","pco","ptn","psl","prb","psol","ppl","pros","psd","sd", "pr","pen", "pcdob", "ptdob","prtb")
b = c("psdb", "aécio", "neves")
targets = data_frame(X = x, Y = y)
features = data_frame(A = a, B = b)
x <- c("pt","psdb","pmdb","ptb","pdt","dem","psb","ptc","psc","pmn","prp","pps","pv","pp","pstu","pcb")
y = c("phs","psdc","pco","ptn","psl","prb","psol","ppl","pros","psd","sd", "pr","pen", "pcdob", "ptdob","prtb")
#x = c("psol","pmdb")
#y = c("psb", "pv")
a = c("pt", "dilma", "rousseff")
b = c("psdb", "aécio", "neves")
targets = data_frame(X = x, Y = y)
features = data_frame(A = a, B = b)
w_wefat <- function(w, features, modelo){
numerador = score_w(w, features, modelo)
s_w = function(modelo, w, col){
features %>% group_by(get(col)) %>%
summarise(s = cosSim_model(modelo, w, get(col)))
}
s_a = s_w(modelo, w, "A")
s_b = s_w(modelo, w, "B")
w_wefat = bind_rows(s_a, s_b) %>% summarise(sd = sd(s)) %>% as.numeric()
return(w_wefat)
}
wefat <- function(targets, features, modelo){
get_w_wefat = function(col){
targets %>% group_by(get(col)) %>% summarise(w_wefat = w_wefat(get(col), features, modelo))
}
w_wefat_x = get_w_wefat("X")
w_wefat_y = get_w_wefat("Y")
w_wefat = bind_rows(w_wefat_x, w_wefat_y) %>% rename(palavra = "get(col)")
return(w_wefat)
}
wefat(targets, features, we_estadao_noticias)
wefat(targets, features, we_estadao_noticias) %>% arrange(w_wefat)
wefat(targets, features, we_estadao_noticias) %>% arrange(-w_wefat)
cosineSimilarity(we_estadao_noticias[["dilma"]],we_estadao_noticias[["aécio"]])
setwd("~/workspace/PhD/src/vies_jornais/analise_descritiva_portais/word2vec/2016-Bolukbasi-MenProgrammerWomenHousemaker")
knitr::opts_chunk$set(echo = TRUE)
source("../../utils/mongo_utils.R")
source("../../utils/utils.R")
source("../../utils/embeddings_utils.R")
source("../Word embeddings/cria_word_embedding.R")
source("../word_embeddings/cria_word_embedding.R")
# limiar de 1 porque eh o cossendo de pi/3 (que implica da distancia entre as palavras serem menores entre si que entre elas e a origem)
limiar = 1.1
MIN_TAMANHO = 3
referencia_1 = "dilma"
referencia_2 = "aécio"
path_saida = "saida_distancia_partidos/"
analogias <- c("pt psdb dilma", "pt pv dilma","pt prtb dilma","pt psol dilma","pt psb dilma","pt psdc dilma","psdb pv aécio","psdb prtb aécio","psdb psol aécio","psdb psb aécio","psdb psdc aécio","pv prtb jorge","pv psol jorge","pv psb jorge","pv psdc jorge","prtb psol fidelix","prtb psb fidelix","prtb psdc fidelix","psol psb luciana","psol psdc luciana","psb psdc marina","dilma aécio rousseff","campos psb aécio","pt psdb petista")
respostas <- c("aécio","jorge","fidelix","luciana","marina","eymael","jorge","fidelix","luciana","marina","eymael","fidelix","luciana","marina","eymael","luciana","marina","eymael","marina","eymael","eymael","neves","psdb","tucano")
partidos <- c("pmdb","ptb","pdt","pt","dem ","psb","psdb","ptc","psc","pmn","prp","pps","pv","pp","pstu","pcb","prtb","phs","psdc","pco","ptn","psl","prb","psol","ppl","pros","psd |psd[.]"," sd |sd[.]") #,"pr","pen", "pcdob", "ptdob"
candidatos <- c("dilma","aécio","levy","marina silva","luciana genro","eduardo jorge","josé maria","pastor everaldo", "iasi","eymael","rui costa","eduardo campos")
entity <- c(candidatos, partidos)
pattern <- paste(entity, collapse = "|")
direct_bias_detection <- function(palavras_normalizadas_usadas, referencia_1, referencia_2, entity){
### Direct bias
g <- (palavras_normalizadas_usadas[[referencia_1]] - palavras_normalizadas_usadas[[referencia_2]])
constant <- 1
direct_bias_words <- direct_bias_calculator(entity, g, constant, palavras_normalizadas_usadas)
direct_bias_value <- direct_bias_words %>% summarise(valor = abs(cosine_c) %>% mean())
### Direct bias graphic
# a coluna y esta sendo usada apenas para dispersar as palavras verticalmente e evitar a sobreposicao delas
direct_bias_words_graphic <- direct_bias_words %>% filter(!(entidade %in% c(referencia_1, referencia_2))) %>% mutate(y = sample(-9:9, n(), replace = T))
direct_bias_words_graphic %>% ggplot(aes(x=cosine_c, y=y, label=entidade)) + geom_text()
ggsave(paste(tema,".pdf",sep=""))
### Indirect bias
#entities <- data_frame(entidade = partidos)
#indirect_bias_psdb <- entities %>% rowwise() %>% mutate(indirect_bias = indirect_bias_calculator(entidade, "psdb", g, palavras_normalizadas_usadas))
return(list(direct_bias_words, direct_bias_value))
}
computa_distancias <- function(modelo, tema, MIN_TAMANHO, referencia_1, referencia_2, entity, path_saida){
### Normaliza vetores
palavras_normalizadas <- modelo %>% apply(1,normaliza_vetor) %>% t() %>% as.VectorSpaceModel()
### Filtra palavras
## filtra palavras a partir de criterio
path = paste("./",train_file,sep="")
palavras_usadas <- palavras_mais_frequentes(path, 50)
palavras_usadas <- c(palavras_usadas[str_length(palavras_usadas) > MIN_TAMANHO], partidos)
palavras_normalizadas_usadas <- palavras_normalizadas[[palavras_usadas, average=F]]
### Distância entre par de palavras
## Para cada par de palavras do conjunto de palavras neutras, calcula a distancia entre elas.
par_palavras <- forma_par_palavras(palavras_normalizadas_usadas)
path_par_palavras <- paste(path_saida,"par_palavras_",tema,".csv",sep="")
write_csv(par_palavras, path_par_palavras)
#par_palavras <- read_csv(path_par_palavras)
proximidade_par_palavras <- calcula_proximidade(par_palavras, modelo, palavras_normalizadas_usadas)
path_proximidade_par_palavras = paste(path_saida,"par_palavras_proximidade_",tema,".csv",sep="")
write_csv(proximidade_par_palavras, path_proximidade_par_palavras)
#proximidade_par_palavras <- read_csv(path_proximidade_par_palavras)
palavras_neutras <- proximidade_par_palavras %>% filter(palavra_1 %in% entity & palavra_2 %in% entity)
### Similaridade entre pares de palavras
similaridade_palavras <- calcula_s_score(proximidade_par_palavras, palavras_normalizadas_usadas, referencia_1, referencia_2)
path_similaridade_par_palavras = paste(path_saida,"par_palavras_similaridade_",tema,".csv",sep="")
write_csv(similaridade_palavras, path_similaridade_par_palavras)
return(list(palavras_normalizadas_usadas, similaridade_palavras))
}
we_noticias_estadao <- read.binary.vectors("../word_embeddings/Eleicoes_2014/embeddings/eleicoes_2014_estadao.bin")
we_noticias_estadao <- read.binary.vectors("../word_embeddings/eleicoes_2014/embeddings/eleicoes_2014_estadao.bin")
cosineSimilarity(we_noticias_estadao[["dilma"]],we_noticias_estadao[["aécio"]])
tema = "estadao_noticias_eleicao_2014_teste_2"
normaliza_palavras <- function(modelo, MIN_TAMANHO){
### Normaliza vetores
palavras_normalizadas <- modelo %>% apply(1,normaliza_vetor) %>% t() %>% as.VectorSpaceModel()
### Filtra palavras
## filtra palavras a partir de criterio
path = paste("./",train_file,sep="")
palavras_usadas <- palavras_mais_frequentes(path, 50)
palavras_usadas <- c(palavras_usadas[str_length(palavras_usadas) > MIN_TAMANHO], partidos)
palavras_normalizadas_usadas <- palavras_normalizadas[[palavras_usadas, average=F]]
return(palavras_normalizadas_usadas)
}
normaliza_palavras <- function(modelo, MIN_TAMANHO){
### Normaliza vetores
palavras_normalizadas <- modelo %>% apply(1,normaliza_vetor) %>% t() %>% as.VectorSpaceModel()
### Filtra palavras
## filtra palavras a partir de criterio
path = paste("./",train_file,sep="")
palavras_usadas <- palavras_mais_frequentes(path, 50)
palavras_usadas <- c(palavras_usadas[str_length(palavras_usadas) > MIN_TAMANHO], partidos)
palavras_normalizadas_usadas <- palavras_normalizadas[[palavras_usadas, average=F]]
return(palavras_normalizadas_usadas)
}
palavra_normalizadas_usadas_estadao_noticias = normaliza_palavras(we_noticias_estadao, MIN_TAMANHO)
knitr::opts_chunk$set(echo = TRUE)
source("../../utils/mongo_utils.R")
source("../../utils/utils.R")
source("../../utils/embeddings_utils.R")
source("../word_embeddings/cria_word_embedding.R")
library("wordVectors")
library("readr")
normaliza_palavras <- function(modelo, MIN_TAMANHO){
### Normaliza vetores
palavras_normalizadas <- modelo %>% apply(1,normaliza_vetor) %>% t() %>% as.VectorSpaceModel()
### Filtra palavras
## filtra palavras a partir de criterio
path = paste("./",train_file,sep="")
palavras_usadas <- palavras_mais_frequentes(path, 50)
palavras_usadas <- c(palavras_usadas[str_length(palavras_usadas) > MIN_TAMANHO], partidos)
palavras_normalizadas_usadas <- palavras_normalizadas[[palavras_usadas, average=F]]
return(palavras_normalizadas_usadas)
}
computa_distancias <- function(modelo, palavra_normalizadas_usadas, tema, referencia_1, referencia_2, entity, path_saida){
### Distância entre par de palavras
## Para cada par de palavras do conjunto de palavras neutras, calcula a distancia entre elas.
par_palavras <- forma_par_palavras(palavras_normalizadas_usadas)
path_par_palavras <- paste(path_saida,"par_palavras_",tema,".csv",sep="")
write_csv(par_palavras, path_par_palavras)
#par_palavras <- read_csv(path_par_palavras)
proximidade_par_palavras <- calcula_proximidade(par_palavras, modelo, palavras_normalizadas_usadas)
path_proximidade_par_palavras = paste(path_saida,"par_palavras_proximidade_",tema,".csv",sep="")
write_csv(proximidade_par_palavras, path_proximidade_par_palavras)
#proximidade_par_palavras <- read_csv(path_proximidade_par_palavras)
palavras_neutras <- proximidade_par_palavras %>% filter(palavra_1 %in% entity & palavra_2 %in% entity)
### Similaridade entre pares de palavras
similaridade_palavras <- calcula_s_score(proximidade_par_palavras, palavras_normalizadas_usadas, referencia_1, referencia_2)
path_similaridade_par_palavras = paste(path_saida,"par_palavras_similaridade_",tema,".csv",sep="")
write_csv(similaridade_palavras, path_similaridade_par_palavras)
return(similaridade_palavras)
}
direct_bias_detection <- function(palavras_normalizadas_usadas, referencia_1, referencia_2, entity){
### Direct bias
g <- (palavras_normalizadas_usadas[[referencia_1]] - palavras_normalizadas_usadas[[referencia_2]])
constant <- 1
direct_bias_words <- direct_bias_calculator(entity, g, constant, palavras_normalizadas_usadas)
direct_bias_value <- direct_bias_words %>% summarise(valor = abs(cosine_c) %>% mean())
### Direct bias graphic
# a coluna y esta sendo usada apenas para dispersar as palavras verticalmente e evitar a sobreposicao delas
direct_bias_words_graphic <- direct_bias_words %>% filter(!(entidade %in% c(referencia_1, referencia_2))) %>% mutate(y = sample(-9:9, n(), replace = T))
direct_bias_words_graphic %>% ggplot(aes(x=cosine_c, y=y, label=entidade)) + geom_text()
ggsave(paste(tema,".pdf",sep=""))
### Indirect bias
#entities <- data_frame(entidade = partidos)
#indirect_bias_psdb <- entities %>% rowwise() %>% mutate(indirect_bias = indirect_bias_calculator(entidade, "psdb", g, palavras_normalizadas_usadas))
return(list(direct_bias_words, direct_bias_value))
}
partidos <- c("pmdb","ptb","pdt","pt","dem ","psb","psdb","ptc","psc","pmn","prp","pps","pv","pp","pstu","pcb","prtb","phs","psdc","pco","ptn","psl","prb","psol","ppl","pros","psd |psd[.]"," sd |sd[.]") #,"pr","pen", "pcdob", "ptdob"
candidatos <- c("dilma","aécio","levy","marina silva","luciana genro","eduardo jorge","josé maria","pastor everaldo", "iasi","eymael","rui costa","eduardo campos")
entity <- c(candidatos, partidos)
pattern <- paste(entity, collapse = "|")
# limiar de 1 porque eh o cossendo de pi/3 (que implica da distancia entre as palavras serem menores entre si que entre elas e a origem)
limiar = 1.1
MIN_TAMANHO = 3
referencia_1 = "dilma"
referencia_2 = "aécio"
path_saida = "saida_distancia_partidos/"
analogias <- c("pt psdb dilma", "pt pv dilma","pt prtb dilma","pt psol dilma","pt psb dilma","pt psdc dilma","psdb pv aécio","psdb prtb aécio","psdb psol aécio","psdb psb aécio","psdb psdc aécio","pv prtb jorge","pv psol jorge","pv psb jorge","pv psdc jorge","prtb psol fidelix","prtb psb fidelix","prtb psdc fidelix","psol psb luciana","psol psdc luciana","psb psdc marina","dilma aécio rousseff","campos psb aécio","pt psdb petista")
respostas <- c("aécio","jorge","fidelix","luciana","marina","eymael","jorge","fidelix","luciana","marina","eymael","fidelix","luciana","marina","eymael","luciana","marina","eymael","marina","eymael","eymael","neves","psdb","tucano")
we_noticias_estadao <- read.binary.vectors("../word_embeddings/eleicoes_2014/embeddings/eleicoes_2014_estadao.bin")
tema = "estadao_noticias_eleicao_2014_teste_2"
palavra_normalizadas_usadas_estadao_noticias = normaliza_palavras(we_noticias_estadao, MIN_TAMANHO)
normaliza_palavras <- function(modelo, MIN_TAMANHO, train_file){
### Normaliza vetores
palavras_normalizadas <- modelo %>% apply(1,normaliza_vetor) %>% t() %>% as.VectorSpaceModel()
### Filtra palavras
## filtra palavras a partir de criterio
path = paste("./",train_file,sep="")
print(path)
palavras_usadas <- palavras_mais_frequentes(path, 50)
palavras_usadas <- c(palavras_usadas[str_length(palavras_usadas) > MIN_TAMANHO], partidos)
palavras_normalizadas_usadas <- palavras_normalizadas[[palavras_usadas, average=F]]
return(palavras_normalizadas_usadas)
}
palavra_normalizadas_usadas_estadao_noticias = normaliza_palavras(we_noticias_estadao, MIN_TAMANHO, "eleicoes_2014_estadao.csv")
normaliza_palavras <- function(modelo, MIN_TAMANHO, train_file){
### Normaliza vetores
palavras_normalizadas <- modelo %>% apply(1,normaliza_vetor) %>% t() %>% as.VectorSpaceModel()
### Filtra palavras
## filtra palavras a partir de criterio
palavras_usadas <- palavras_mais_frequentes(train_file, 50)
palavras_usadas <- c(palavras_usadas[str_length(palavras_usadas) > MIN_TAMANHO], partidos)
palavras_normalizadas_usadas <- palavras_normalizadas[[palavras_usadas, average=F]]
return(palavras_normalizadas_usadas)
}
palavra_normalizadas_usadas_estadao_noticias = normaliza_palavras(we_noticias_estadao, MIN_TAMANHO, "../word_embeddings/eleicoes_2014/embeddings/eleicoes_2014_estadao.csv")
direct_bias_detection(palavra_normalizadas_usadas_estadao_noticias, referencia_1, referencia_2, entity)
direct_bias_detection <- function(palavras_normalizadas_usadas, tema, referencia_1, referencia_2, entity){
### Direct bias
g <- (palavras_normalizadas_usadas[[referencia_1]] - palavras_normalizadas_usadas[[referencia_2]])
constant <- 1
direct_bias_words <- direct_bias_calculator(entity, g, constant, palavras_normalizadas_usadas)
direct_bias_value <- direct_bias_words %>% summarise(valor = abs(cosine_c) %>% mean())
### Direct bias graphic
# a coluna y esta sendo usada apenas para dispersar as palavras verticalmente e evitar a sobreposicao delas
direct_bias_words_graphic <- direct_bias_words %>% filter(!(entidade %in% c(referencia_1, referencia_2))) %>% mutate(y = sample(-9:9, n(), replace = T))
direct_bias_words_graphic %>% ggplot(aes(x=cosine_c, y=y, label=entidade)) + geom_text()
ggsave(paste(tema,".pdf",sep=""))
### Indirect bias
#entities <- data_frame(entidade = partidos)
#indirect_bias_psdb <- entities %>% rowwise() %>% mutate(indirect_bias = indirect_bias_calculator(entidade, "psdb", g, palavras_normalizadas_usadas))
return(list(direct_bias_words, direct_bias_value))
}
we_comentarios_folha <- read.binary.vectors("../word_embeddings/eleicoes_2014/embeddings/folha_comentarios_eleicao_2014.bin")
palavra_normalizadas_usadas_folha_comentarios = normaliza_palavras(we_comentarios_folha, MIN_TAMANHO, "../word_embeddings/eleicoes_2014/embeddings/folha_comentarios_eleicao_2014.csv")
direct_bias_detection(palavra_normalizadas_usadas_folha_comentarios, tema, referencia_1, referencia_2, entity)
setwd("~/workspace/PhD/src/vies_jornais/word2vec/2016-Bolukbasi-MenProgrammerWomenHousemaker")
source("../../utils/mongo_utils.R")
source("../word_embeddings/cria_word_embedding.R")
